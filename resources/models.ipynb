{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyP54g4mmuBl+I/XIEid0Sm6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"khP6NWrdKWSk","executionInfo":{"status":"ok","timestamp":1670385839495,"user_tz":480,"elapsed":18114,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"dfc05451-c834-4a69-cbb8-47644aac6fd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["!pip install scanpy --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZFSNox2KiKN","executionInfo":{"status":"ok","timestamp":1670385859306,"user_tz":480,"elapsed":19817,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"6654e89f-b1cf-4bf7-fe69-98a400cf5ff0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 2.0 MB 14.4 MB/s \n","\u001b[K     |████████████████████████████████| 88 kB 8.4 MB/s \n","\u001b[K     |████████████████████████████████| 96 kB 5.7 MB/s \n","\u001b[K     |████████████████████████████████| 9.4 MB 51.2 MB/s \n","\u001b[K     |████████████████████████████████| 295 kB 81.9 MB/s \n","\u001b[K     |████████████████████████████████| 965 kB 83.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 64.2 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n","\u001b[?25h  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7bo5YGlE7wI6"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.autograd import Variable\n","import anndata as ad\n","import numpy as np\n","import os\n","from argparse import Namespace\n","config = Namespace(\n","    NUM_WORKERS = 4,\n","    N_GENES = 13431,\n","    N_PEAKS = 116465,\n","    N_CHANNELS = 32,\n","    MAX_SEQ_LEN_GEX = 1500,\n","    MAX_SEQ_LEN_ATAC = 15000\n",")"]},{"cell_type":"code","source":["adata_gex = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\")"],"metadata":{"id":"xDWcM83eKMAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["adata_atac = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/ATAC_processed.h5ad\")"],"metadata":{"id":"HyD0b7IGp412"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_chr_index(adata_atac):\n","  r\"\"\"\n","  Output row indices for each chromosome for each chromosome\n","  Parameters\n","  ----------\n","  adata_atac\n","      annData for ATAC\n","  Returns\n","  -------\n","  chr_index\n","      Dictionary of indices for each chromosome\n","  \"\"\"\n","  row_name = adata_atac.var.index\n","  chr_name = [c.split(\"-\")[0] for c in row_name]\n","  lst = np.unique(chr_name) # names for chromosome\n","\n","  chr_index = dict()\n","  for i in range(len(lst)):\n","    index = [a for a, l in enumerate(chr_name) if l == lst[i]]\n","    if lst[i] not in chr_index:\n","      chr_index[lst[i]]=index\n","\n","  return chr_index"],"metadata":{"id":"rcYnPeRzsPLH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Write cnn modules for gex modalities\n","class gexCNN(nn.Module):\n","    \"\"\"customized  module\"\"\"\n","    #argument index is the poisition for each choromosome\n","    def __init__(self, kernel_size):\n","        super(gexCNN, self).__init__()\n","\n","        # Conv layer\n","        self.in_channels = 1 \n","        self.out_channels = config.N_CHANNELS\n","        self.kernel_size = kernel_size   \n","        self.stride = 10 # TO CHANGE \n","        self.padding = 10 # TO CHANGE\n","        self.pool_size = 2\n","        self.pool_stride = 1\n","        self.convs = nn.Sequential(\n","            nn.Conv1d(in_channels = self.in_channels, \n","                      out_channels = self.out_channels, \n","                      kernel_size = self.kernel_size,\n","                      stride = self.stride,\n","                      padding = self.padding),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size = self.pool_size,\n","                         stride = self.pool_stride)\n","        )\n","\n","        # # FC layer\n","        # self.conv_out_features = int((config.N_GENES + 2*self.padding - self.kernel_size) / self.stride + 1)\n","        # self.fc_in_features = int((self.conv_out_features - self.pool_size) / self.pool_stride + 1) * self.out_channels\n","        # self.fc_out_feature = 300\n","        # self.fc = nn.Linear(in_features = self.fc_in_features, out_features = self.fc_out_feature) \n","\n","    def forward(self, x):\n","        r\"\"\"  \n","        Generate GEX embeddings\n","        \n","        Parameters\n","        ----------\n","        x\n","            Pre-processed GEX data (batch_size x 1 x N_GENES)\n","        \n","        Returns\n","        -------\n","        gex_embed\n","            GEX embeddings of a batch (batch_size x seq_len x dim_size)\n","        \"\"\"\n","        gex_embed = self.convs(x)\n","        # gex_embed = torch.flatten(gex_embed, 1)\n","        # gex_embed = self.fc(gex_embed)\n","        return gex_embed.transpose(1,2)"],"metadata":{"id":"FlJMBCpbKIEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test for gexCNN()\n","x = torch.tensor(np.asarray(adata_gex.layers['log_norm'][:5].todense())).unsqueeze(1) # 5 cells\n","print(x.size())\n","model = gexCNN(kernel_size = 10)\n","print(model(x).size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Fd0OvPg3PGo","executionInfo":{"status":"ok","timestamp":1670299299488,"user_tz":480,"elapsed":356,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"12b6e85c-e56e-4684-c605-6ea9d3b9a265"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 1, 13431])\n","torch.Size([5, 1344, 32])\n"]}]},{"cell_type":"code","source":["# Write cnn modules for atac modalities\n","class atacCNN(nn.Module):\n","    #argument index is the poisition for each choromosome\n","    def __init__(self, index, kernel_size):\n","        super(atacCNN, self).__init__()\n","        self.index = index\n","        \n","        # Conv layer\n","        self.in_channels = 1 \n","        self.out_channels = config.N_CHANNELS\n","        self.kernel_size = kernel_size   \n","        self.stride = 10 # TO CHANGE \n","        self.padding = 10 # TO CHANGE\n","        self.pool_size = 2\n","        self.pool_stride = 1\n","        self.convs = nn.Sequential(\n","            nn.Conv1d(in_channels = self.in_channels, \n","                      out_channels = self.out_channels, \n","                      kernel_size = self.kernel_size,\n","                      stride = self.stride,\n","                      padding = self.padding),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size = self.pool_size,\n","                         stride = self.pool_stride)\n","        )\n","\n","\n","    def forward(self, x):\n","        r\"\"\"  \n","        Generate ATAC embeddings\n","        \n","        Parameters\n","        ----------\n","        x\n","            Pre-processed ATAC data (batch_size x 1 x N_PEAKS)\n","        \n","        Returns\n","        -------\n","        atac_embed\n","            ATAC embeddings of a batch (batch_size x seq_len x dim_size)\n","        \"\"\"\n","        atac_embed = []\n","        for chr in self.index.keys(): \n","            idx = self.index[chr]\n","            x_chr = x[:,:,idx]\n","            x_chr = self.convs(x_chr.float())\n","            atac_embed.append(x_chr)\n","        atac_embed = torch.cat(atac_embed, dim = 2)\n","        return atac_embed.transpose(1,2)"],"metadata":{"id":"LUWvJUcpKIHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test for ATAC_CNN()\n","x = torch.tensor(np.asarray(adata_atac.layers['log_norm'][:5].todense())).unsqueeze(1) # 5 cells\n","print(x.size())\n","index = get_chr_index(adata_atac)\n","model = atacCNN(kernel_size = 50, index = index)\n","print(model(x).size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"orbL3zsTqYPy","executionInfo":{"status":"ok","timestamp":1670299369081,"user_tz":480,"elapsed":2020,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"3c869af8-a808-41a0-8236-7c95868be822"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 1, 116465])\n","torch.Size([5, 11566, 32])\n"]}]},{"cell_type":"code","source":["class MultimodalAttention(nn.Module):\n","    def __init__(self):\n","        super(MultimodalAttention, self).__init__()\n","        self.nhead_gex = 1\n","        self.nhead_atac = 4\n","        self.nhead_multi = 4\n","        self.nlayer_gex = 1\n","        self.nlayer_atac = 1\n","        self.nlayer_multi = 1\n","\n","        self.encoder_layer_gex = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_gex)\n","        self.transformer_encoder_gex = nn.TransformerEncoder(self.encoder_layer_gex, num_layers = self.nlayer_gex)\n","        # self.linear_gex = nn.Linear(in_features = config.MAX_SEQ_LEN_GEX, out_features = 1)\n","\n","        self.encoder_layer_atac = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_atac)\n","        self.transformer_encoder_atac = nn.TransformerEncoder(self.encoder_layer_atac, num_layers = self.nlayer_atac)\n","        # self.linear_atac = nn.Linear(in_features = config.MAX_SEQ_LEN_ATAC, out_features = 1)\n","\n","        self.encoder_layer_multi = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_multi)\n","        self.transformer_encoder_multi = nn.TransformerEncoder(self.encoder_layer_multi, num_layers = self.nlayer_multi)\n","    \n","\n","    def forward(self, gex_embed, atac_embed):\n","      r\"\"\"  \n","      Incorporate two self-attention and one cross-attention module\n","\n","      Parameters\n","      ----------\n","      gex_embed\n","          GEX embeddings of a batch (batch_size x seq_len_gex x dim_size)\n","      atac_embed\n","          ATAC embeddings of a batch (batch_size x seq_len_atac x dim_size)\n","\n","      Returns\n","      -------\n","      ## TO FILL\n","      \"\"\"\n","      seq_len_gex = gex_embed.size()[1]\n","      seq_len_atac = atac_embed.size()[1]\n","\n","      gex_context = self.transformer_encoder_gex(gex_embed)\n","      atac_context = self.transformer_encoder_atac(atac_embed)\n","      print(gex_context.size())\n","      print(atac_context.size())\n","\n","      # Average self-attention fragment representation\n","      gex_out_0 = gex_context.mean(dim = 1)\n","      atac_out_0 = atac_context.mean(dim = 1)\n","\n","      multi_embed = torch.cat((gex_context, atac_context), dim = 1)\n","      multi_context = self.transformer_encoder_multi(multi_embed)\n","      print(multi_context.size())\n","      \n","      multi_context_gex = multi_context[:, :seq_len_gex, :]\n","      multi_context_atac = multi_context[:, seq_len_gex:, :]\n","\n","      # Average cross-attention fragment representation\n","      gex_out_1 = multi_context_gex.mean(dim = 1)\n","      atac_out_1 = multi_context_atac.mean(dim = 1)\n","\n","      return gex_out_0, gex_out_1, atac_out_0, atac_out_1"],"metadata":{"id":"2cTz8oUD4aGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index = get_chr_index(adata_atac)\n","\n","x_gex = torch.tensor(np.asarray(adata_gex.layers['log_norm'][:5].todense())).unsqueeze(1) # 5 cells\n","x_atac = torch.tensor(np.asarray(adata_atac.layers['log_norm'][:5].todense())).unsqueeze(1) # 5 cells\n","\n","gex_cnn = gexCNN(kernel_size = 10)\n","atac_cnn = atacCNN(kernel_size = 20, index = index)\n","multi_attention = MultimodalAttention()\n","\n","gex_embed = gex_cnn(x_gex)\n","atac_embed = atac_cnn(x_atac)\n","\n","gex_out_0, gex_out_1, atac_out_0, atac_out_1 = multi_attention(gex_embed, atac_embed)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bKfe5KqplRBE","executionInfo":{"status":"ok","timestamp":1670301460534,"user_tz":480,"elapsed":3155,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"a23ab174-62f6-45fe-e618-aa3a08013219"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 1344, 32])\n","torch.Size([5, 11635, 32])\n","torch.Size([5, 12979, 32])\n"]}]},{"cell_type":"code","source":["class Custom_MSE(nn.Module):\n","  def __init__(self):\n","    super(Custom_MSE, self).__init__();\n","\n","  def forward(self, predictions, target):\n","    square_difference = torch.square(predictions - target)\n","    loss_value = torch.mean(square_difference)\n","    return loss_value\n","  \n","class bidirectTripletLoss(nn.Module):\n","    r\"\"\"\n","    \n","    Output bidirectional triplet loss for two pairs of gex and atac\n","    ----------\n","    gex_0_mat: Matrix of GEX embeddings from self-attention (batch_size x embedding_size_0)\n","\n","    Returns\n","    -------\n","    loss\n","    \"\"\"\n","    def __init__(self, alpha, margin):\n","        super(bidirectTripletLoss, self).__init__()\n","        self.alpha = alpha\n","        self.margin = margin\n","        self.loss = nn.TripletMarginWithDistanceLoss(distance_function = self.similarityScore, margin = self.margin)\n","\n","    def similarityScore(self, gex_vec, atac_vec):\n","        r\"\"\"\n","        Output similarity scores for two pairs of gex and atac\n","        ----------\n","        Vector of gex and atac embeddings \n","        gex_vec: (1 x 2*config.N_CHANNELS)\n","        atac_vec: (1 x 2*config.N_CHANNELS)\n","\n","        Returns\n","        -------\n","        similarity score between two modalities\n","        \"\"\"\n","        score = torch.dot(gex_vec[:config.N_CHANNELS], atac_vec[:config.N_CHANNELS]) + self.alpha * torch.dot(gex_vec[config.N_CHANNELS:], atac_vec[config.N_CHANNELS:])\n","        return  score\n","\n","    def forward(self, gex_mat, atac_mat):\n","        r\"\"\"\n","        Output bi-directional triplet ranking scores for two pairs of gex and atac\n","        ----------\n","        Matrix of gex and atac embeddings \n","        gex_mat: (batch_size x 2*config.N_CHANNELS)\n","        atac_mat: (batch_size x 2*config.N_CHANNELS)\n","\n","        Returns\n","        -------\n","        Bi-directional triplet ranking scores between two modalities\n","        \"\"\"\n","        gex_mat_0, gex_mat_1 = torch.split(gex_mat, 1)\n","        atac_mat_0, atac_mat_1 = torch.split(atac_mat, 1)\n","\n","        similarity_matrix = torch.matmul(gex_mat_0, atac_mat_0) + self.alpha * torch.matmul(gex_mat_1, atac_mat_1)\n","        \n","\n","        output = loss(anchor, positive, negative) + loss(anchor, positive2, negative2)"],"metadata":{"id":"GPTMaJt6mIL1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding = nn.Embedding(1000, 128)\n","anchor_ids = torch.randint(0, 1000, (1,))\n","positive_ids = torch.randint(0, 1000, (1,))\n","negative_ids = torch.randint(0, 1000, (1,))\n","anchor = embedding(anchor_ids)\n","positive = embedding(positive_ids)\n","negative = embedding(negative_ids)"],"metadata":{"id":"jpHCCxVo-1Bj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(anchor.size())\n","print(positive.size())\n","print(negative.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5oscyv8g_gPF","executionInfo":{"status":"ok","timestamp":1670389917328,"user_tz":480,"elapsed":254,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"14d20744-5aa4-4187-cde0-7edbdb03dc24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 128])\n","torch.Size([1, 128])\n","torch.Size([1, 128])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","  \n","class bidirectTripletLoss(nn.Module):\n","    r\"\"\"\n","    \n","    Output bidirectional triplet loss for two pairs of gex and atac\n","    ----------\n","    gex_0_mat: Matrix of GEX embeddings from self-attention (batch_size x embedding_size_0)\n","\n","    Returns\n","    -------\n","    loss\n","    \"\"\"\n","    def __init__(self, alpha, margin):\n","        super(bidirectTripletLoss, self).__init__()\n","\n","        self.alpha = alpha\n","        self.margin = margin\n","\n","    def similarityScore(self, gex_mat, atac_mat):\n","        r\"\"\"\n","        Output similarity scores for two pairs of gex and atac\n","        ----------\n","        list: of gex and atac embeddings \n","        gex_mat:  batch_size * (embed_size * 2)\n","        atac_mat: batch_size * (embed_size * 2)\n","\n","        Returns\n","        -------\n","        score: batch_size * batch_size\n","        similarity score between two modalities\n","        \"\"\"\n","        gex_mat0, gex_mat1 = torch.split(gex_mat, config.N_CHANNELS, dim = 1)\n","        atac_mat0, atac_mat1 = torch.split(atac_mat,config.N_CHANNELS, dim = 1)\n","\n","        score = torch.mm(gex_mat0,atac_mat0.transpose(0,1)) + self.alpha * torch.mm(gex_mat1,atac_mat1.transpose(0,1))\n","\n","        return  score\n","\n","\n","    def forward(self, gex_mat, atac_mat):\n","      \n","        batch_size = gex_mat.size()[0]\n","        score_mat = self.similarityScore(gex_mat, atac_mat)#; print(\"score_mat:\\n\", score_mat)\n","        true_score = torch.diagonal(score_mat)#; print(\"true_score:\\n\", true_score)\n","        \n","        reduced_score_mat = score_mat - torch.diag(true_score) # set the diagnoal score to be zero\n","        \n","        neg_index_1 = torch.argmax(reduced_score_mat, dim = 1) # indices of hard negatives for GEX\n","        neg_index_2 = torch.argmax(reduced_score_mat, dim = 0) # indices of hard negatives for ATAC\n","        # print(\"neg_index_1:\\n\", neg_index_1); print(\"neg_index_2:\\n\", neg_index_2)\n","        neg_1 = score_mat[[range(batch_size), neg_index_1]] # hard negatives for GEX \n","        neg_2 = score_mat[[neg_index_2, range(batch_size)]] # hard negatives for ATAC\n","        # print(\"neg_1:\\n\", neg_1); print(\"neg_2:\\n\", neg_2)\n","        \n","        loss_1 = torch.max(self.margin - true_score + neg_1, torch.zeros(1, batch_size))\n","        loss_2 = torch.max(self.margin - true_score + neg_2, torch.zeros(1, batch_size))\n"," \n","        loss = loss_1 + loss_2\n","\n","        return torch.mean(loss) "],"metadata":{"id":"Aq9aH7Nc_d51"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","random.seed(0)\n","gex_mat=torch.randn([5,64])\n","# print(gex_mat[0,])\n","atac_mat=torch.randn([5,64])\n","# print(atac_mat[0,])\n","# print(torch.dot(gex_mat[0,:32],atac_mat[0,:32]))\n","loss=bidirectTripletLoss(alpha=0.2,margin=2)\n","res=loss(gex_mat,atac_mat)\n","res"],"metadata":{"id":"B3QiH33JZGL3"},"execution_count":null,"outputs":[]}]}