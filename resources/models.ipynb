{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1182,"status":"ok","timestamp":1670991119610,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"khP6NWrdKWSk","outputId":"3902aee8-0592-4f60-c3ca-9fbbc6ff0c9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"SZFSNox2KiKN","executionInfo":{"status":"ok","timestamp":1670991123219,"user_tz":480,"elapsed":3614,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}}},"outputs":[],"source":["!pip install scanpy --quiet"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5478,"status":"ok","timestamp":1670991128690,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"7bo5YGlE7wI6"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.autograd import Variable\n","import anndata as ad\n","import numpy as np\n","import os\n","import collections \n","from argparse import Namespace\n","from torch.utils.data import Dataset, DataLoader\n","config = Namespace(\n","    DEVICE = 'cuda',\n","    BATCH_SIZE = 64,\n","    NUM_WORKERS = 4,\n","    N_GENES = 13431,\n","    N_PEAKS = 116465,\n","    N_CHANNELS = 64,\n","    MAX_SEQ_LEN_GEX = 1500,\n","    MAX_SEQ_LEN_ATAC = 15000,\n","    LEARNING_RATE = 0.00005\n",")"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1670991128691,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"xDWcM83eKMAJ"},"outputs":[],"source":["# adata_gex = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670991128691,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"HyD0b7IGp412"},"outputs":[],"source":["# adata_atac = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/ATAC_processed.h5ad\")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670991128692,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"rcYnPeRzsPLH"},"outputs":[],"source":["def get_chr_index(adata_atac):\n","  r\"\"\"\n","  Output row indices for each chromosome for each chromosome\n","  Parameters\n","  ----------\n","  adata_atac\n","      annData for ATAC\n","  Returns\n","  -------\n","  chr_index\n","      Dictionary of indices for each chromosome\n","  \"\"\"\n","  row_name = adata_atac.var.index\n","  chr_name = [c.split(\"-\")[0] for c in row_name]\n","  lst = np.unique(chr_name) # names for chromosome\n","\n","  chr_index = dict()\n","  for i in range(len(lst)):\n","    index = [a for a, l in enumerate(chr_name) if l == lst[i]]\n","    if lst[i] not in chr_index:\n","      chr_index[lst[i]]=index\n","\n","  return chr_index"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670991128692,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"FlJMBCpbKIEa"},"outputs":[],"source":["## Write cnn modules for gex modalities\n","class gexCNN(nn.Module):\n","    \"\"\"customized  module\"\"\"\n","    #argument index is the poisition for each choromosome\n","    def __init__(self, kernel_size):\n","        super(gexCNN, self).__init__()\n","\n","        # Conv layer\n","        self.in_channels = 1 \n","        self.out_channels = config.N_CHANNELS\n","        self.kernel_size = kernel_size   \n","        self.stride = 50 # TO CHANGE \n","        self.padding = 25 # TO CHANGE\n","        self.pool_size = 2\n","        self.pool_stride = 1\n","        self.convs = nn.Sequential(\n","            nn.Conv1d(in_channels = self.in_channels, \n","                      out_channels = self.out_channels, \n","                      kernel_size = self.kernel_size,\n","                      stride = self.stride,\n","                      padding = self.padding),\n","            nn.LeakyReLU(),\n","            nn.MaxPool1d(kernel_size = self.pool_size,\n","                         stride = self.pool_stride)\n","        )\n","\n","        # # FC layer\n","        # self.conv_out_features = int((config.N_GENES + 2*self.padding - self.kernel_size) / self.stride + 1)\n","        # self.fc_in_features = int((self.conv_out_features - self.pool_size) / self.pool_stride + 1) * self.out_channels\n","        # self.fc_out_feature = 300\n","        # self.fc = nn.Linear(in_features = self.fc_in_features, out_features = self.fc_out_feature) \n","\n","    def forward(self, x):\n","        r\"\"\"  \n","        Generate GEX embeddings\n","        \n","        Parameters\n","        ----------\n","        x\n","            Pre-processed GEX data (batch_size x 1 x N_GENES)\n","        \n","        Returns\n","        -------\n","        gex_embed\n","            GEX embeddings of a batch (batch_size x seq_len x dim_size)\n","        \"\"\"\n","        gex_embed = self.convs(x.float())\n","        # gex_embed = torch.flatten(gex_embed, 1)\n","        # gex_embed = self.fc(gex_embed)\n","        return gex_embed.transpose(1,2).to(config.DEVICE)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670991128692,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"_Fd0OvPg3PGo"},"outputs":[],"source":["# # Test for gexCNN()\n","# x = torch.tensor(np.asarray(csr_gex[:5].todense())).unsqueeze(1) # 5 cells\n","# print(x.size())\n","# model = gexCNN(kernel_size = 100)\n","# print(model(x).size())"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670991128693,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"LUWvJUcpKIHe"},"outputs":[],"source":["# Write cnn modules for atac modalities\n","class atacCNN(nn.Module):\n","    #argument index is the poisition for each choromosome\n","    def __init__(self, index, kernel_size_1, kernel_size_2):\n","        super(atacCNN, self).__init__()\n","        self.index = index\n","        \n","        # Conv layer\n","        self.in_channels_1 = 1 \n","        self.out_channels_1 = int(config.N_CHANNELS / 2)\n","        self.kernel_size_1 = kernel_size_1\n","        self.stride_1 = 10 # TO CHANGE \n","        self.padding_1 = 5 # TO CHANGE\n","\n","        self.in_channels_2 = int(config.N_CHANNELS / 2)\n","        self.out_channels_2 = config.N_CHANNELS \n","        self.kernel_size_2 = kernel_size_2\n","        self.stride_2 = 5 # TO CHANGE \n","        self.padding_2 = 3 # TO CHANGE\n","\n","        self.convs = nn.Sequential(\n","            nn.Conv1d(in_channels = self.in_channels_1, \n","                      out_channels = self.out_channels_1, \n","                      kernel_size = self.kernel_size_1,\n","                      stride = self.stride_1,\n","                      padding = self.padding_1),\n","            nn.LeakyReLU(),\n","            nn.MaxPool1d(kernel_size = 5, stride = 2),\n","\n","            nn.Conv1d(in_channels = self.in_channels_2, \n","                      out_channels = self.out_channels_2, \n","                      kernel_size = self.kernel_size_2,\n","                      stride = self.stride_2,\n","                      padding = self.padding_2),\n","            nn.LeakyReLU(),\n","            nn.MaxPool1d(kernel_size = 2, stride = 1)             \n","        )\n","\n","\n","\n","    def forward(self, x):\n","        r\"\"\"  \n","        Generate ATAC embeddings\n","        \n","        Parameters\n","        ----------\n","        x\n","            Pre-processed ATAC data (batch_size x 1 x N_PEAKS)\n","        \n","        Returns\n","        -------\n","        atac_embed\n","            ATAC embeddings of a batch (batch_size x seq_len x dim_size)\n","        \"\"\"\n","        atac_embed = []\n","        for chr in self.index.keys(): \n","            idx = self.index[chr]\n","            x_chr = x[:,:,idx]\n","            x_chr = self.convs(x_chr.float())\n","            atac_embed.append(x_chr)\n","        atac_embed = torch.cat(atac_embed, dim = 2)\n","        return atac_embed.transpose(1,2).to(config.DEVICE)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1670991128694,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"orbL3zsTqYPy"},"outputs":[],"source":["# # Test for ATAC_CNN()\n","# x = torch.tensor(np.asarray(csr_atac[:5].todense())).unsqueeze(1) # 5 cells\n","# print(x.size())\n","# # index = get_chr_index(adata_atac)\n","# model = atacCNN(kernel_size_1 = 50, kernel_size_2 = 10, index = index)\n","# print(model(x).size())"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1670991128694,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"2cTz8oUD4aGx"},"outputs":[],"source":["class MultimodalAttention(nn.Module):\n","    def __init__(self):\n","        super(MultimodalAttention, self).__init__()\n","        self.nhead_gex = 1\n","        self.nhead_atac = 4\n","        self.nhead_multi = 4\n","        self.nlayer_gex = 1\n","        self.nlayer_atac = 1\n","        self.nlayer_multi = 1\n","\n","        self.encoder_layer_gex = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_gex)\n","        self.transformer_encoder_gex = nn.TransformerEncoder(self.encoder_layer_gex, num_layers = self.nlayer_gex)\n","        self.linear_gex_0 = nn.LazyLinear(out_features = 1)\n","\n","        self.encoder_layer_atac = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_atac)\n","        self.transformer_encoder_atac = nn.TransformerEncoder(self.encoder_layer_atac, num_layers = self.nlayer_atac)\n","        self.linear_atac_0 = nn.LazyLinear(out_features = 1)\n","\n","        self.encoder_layer_multi = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_multi)\n","        self.transformer_encoder_multi = nn.TransformerEncoder(self.encoder_layer_multi, num_layers = self.nlayer_multi)\n","        self.linear_gex_1 = nn.LazyLinear(out_features = 1)\n","        self.linear_atac_1 = nn.LazyLinear(out_features = 1)\n","    \n","\n","    def forward(self, gex_embed, atac_embed):\n","      r\"\"\"  \n","      Incorporate two self-attention and one cross-attention module\n","\n","      Parameters\n","      ----------\n","      gex_embed\n","          GEX embeddings of a batch (batch_size x seq_len_gex x dim_size)\n","      atac_embed\n","          ATAC embeddings of a batch (batch_size x seq_len_atac x dim_size)\n","\n","      Returns\n","      -------\n","      ## TO FILL\n","      \"\"\"\n","      seq_len_gex = gex_embed.size()[1]\n","      seq_len_atac = atac_embed.size()[1]\n","      # print(gex_embed.size())\n","      # print(atac_embed.size())\n","\n","      gex_context = self.transformer_encoder_gex(gex_embed)\n","      atac_context = self.transformer_encoder_atac(atac_embed)\n","\n","      # # Average self-attention fragment representation\n","      # gex_out_0 = gex_context.mean(dim = 1)\n","      # atac_out_0 = atac_context.mean(dim = 1)\n","      gex_out_0 = self.linear_gex_0(gex_context.permute(0,2,1)).squeeze(2)\n","      atac_out_0 = self.linear_atac_0(atac_context.permute(0,2,1)).squeeze(2)\n","\n","      multi_embed = torch.cat((gex_context, atac_context), dim = 1)\n","      multi_context = self.transformer_encoder_multi(multi_embed)\n","      # print(multi_context.size())\n","      \n","      multi_context_gex = multi_context[:, :seq_len_gex, :]\n","      multi_context_atac = multi_context[:, seq_len_gex:, :]\n","\n","      # # Average cross-attention fragment representation\n","      # gex_out_1 = multi_context_gex.mean(dim = 1)\n","      # atac_out_1 = multi_context_atac.mean(dim = 1)\n","      gex_out_1 = self.linear_gex_1(multi_context_gex.permute(0,2,1)).squeeze(2)\n","      atac_out_1 = self.linear_atac_1(multi_context_atac.permute(0,2,1)).squeeze(2)\n","\n","      return gex_out_0.to(config.DEVICE), gex_out_1.to(config.DEVICE), atac_out_0.to(config.DEVICE), atac_out_1.to(config.DEVICE)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1670991128695,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"bKfe5KqplRBE"},"outputs":[],"source":["# # index = get_chr_index(adata_atac)\n","\n","# x_gex = torch.tensor(np.asarray(csr_gex[:5].todense())).unsqueeze(1).to(config.DEVICE) # 5 cells\n","# x_atac = torch.tensor(np.asarray(csr_atac[:5].todense())).unsqueeze(1).to(config.DEVICE) # 5 cells\n","\n","# gex_cnn = gexCNN(kernel_size = 100).to(config.DEVICE)\n","# atac_cnn = atacCNN(kernel_size_1 = 50, kernel_size_2 = 10, index = index).to(config.DEVICE)\n","# multi_attention = MultimodalAttention().to(config.DEVICE)\n","\n","# gex_embed = gex_cnn(x_gex).to(config.DEVICE)\n","# atac_embed = atac_cnn(x_atac).to(config.DEVICE)\n","\n","# gex_out_0, gex_out_1, atac_out_0, atac_out_1 = multi_attention(gex_embed, atac_embed)\n","# print(atac_out_0.size())"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1670991128695,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"jpHCCxVo-1Bj"},"outputs":[],"source":["# embedding = nn.Embedding(1000, 128)\n","# anchor_ids = torch.randint(0, 1000, (1,))\n","# positive_ids = torch.randint(0, 1000, (1,))\n","# negative_ids = torch.randint(0, 1000, (1,))\n","# anchor = embedding(anchor_ids)\n","# positive = embedding(positive_ids)\n","# negative = embedding(negative_ids)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670991128695,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"5oscyv8g_gPF"},"outputs":[],"source":["# print(anchor.size())\n","# print(positive.size())\n","# print(negative.size())"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1670991128696,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"ZXU1s_kVtKUQ"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, kernel_size_gex, kernel_size_atac_1, kernel_size_atac_2, index):\n","        super(Encoder, self).__init__()\n","\n","        self.kernel_size_gex = kernel_size_gex\n","        self.kernel_size_atac_1 = kernel_size_atac_1\n","        self.kernel_size_atac_2 = kernel_size_atac_2\n","        self.index = index\n","\n","        self.gex_cnn = gexCNN(kernel_size = self.kernel_size_gex)\n","        self.atac_cnn = atacCNN(kernel_size_1 = self.kernel_size_atac_1, kernel_size_2 = self.kernel_size_atac_2, index = self.index)\n","        self.multi_attention = MultimodalAttention()\n","\n","        \n","    def forward(self, x_gex, x_atac):\n","\n","        gex_embed = self.gex_cnn(x_gex)\n","        atac_embed = self.atac_cnn(x_atac)\n","        gex_out_0, gex_out_1, atac_out_0, atac_out_1 = self.multi_attention(gex_embed, atac_embed)\n","\n","        return gex_out_0, gex_out_1, atac_out_0, atac_out_1\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1670991128696,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"Ql7ebMHlvjho"},"outputs":[],"source":["# gex_test = torch.tensor(np.asarray(adata_gex.layers['log_norm'][:5].todense())).unsqueeze(1) # 5 cells\n","# atac_test = torch.tensor(np.asarray(adata_atac.layers['log_norm'][:5].todense())).unsqueeze(1) # 5 cells\n","\n","# index = get_chr_index(adata_atac)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670991128696,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"O8JVH48uxfQU"},"outputs":[],"source":["# encoder = Encoder(kernel_size = 32, index = index )\n","# gex_out_0, gex_out_1, atac_out_0, atac_out_1 = encoder(gex_test, atac_test)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1670991128913,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"Aq9aH7Nc_d51"},"outputs":[],"source":["from numpy.lib.shape_base import row_stack\n","class bidirectTripletLoss(nn.Module):\n","    r\"\"\"\n","    \n","    Output bidirectional triplet loss for two pairs of gex and atac\n","    ----------\n","    gex_0_mat: Matrix of GEX embeddings from self-attention (batch_size x embedding_size_0)\n","\n","    Returns\n","    -------\n","    loss\n","    \"\"\"\n","    def __init__(self, alpha, margin):\n","        super(bidirectTripletLoss, self).__init__()\n","\n","        self.alpha = alpha\n","        self.margin = margin\n","        self.cross_entropy_loss = nn.CrossEntropyLoss()\n","\n","    def similarityScore(self, gex_out_0, gex_out_1, atac_out_0, atac_out_1):\n","        r\"\"\"\n","        Output similarity scores for two pairs of gex and atac\n","        ----------\n","\n","\n","        Returns\n","        -------\n","        score: batch_size * batch_size\n","        similarity score between two modalities\n","        \"\"\" \n","\n","        # print(gex_mat.size())\n","        # print(atac_mat.size())\n","\n","        # gex_mat0, gex_mat1 = torch.split(gex_mat, config.N_CHANNELS, dim = 1)\n","        # atac_mat0, atac_mat1 = torch.split(atac_mat,config.N_CHANNELS, dim = 1)\n","\n","        gex_out_0 = nn.functional.normalize(gex_out_0, dim = 1)\n","        gex_out_1 = nn.functional.normalize(gex_out_1, dim = 1)\n","        atac_out_0 = nn.functional.normalize(atac_out_0, dim = 1)\n","        atac_out_1 = nn.functional.normalize(atac_out_1, dim = 1)\n","        \n","        score = torch.mm(gex_out_0, atac_out_0.transpose(0,1)) + self.alpha * torch.mm(gex_out_1, atac_out_1.transpose(0,1))\n","        return  score.to(config.DEVICE)\n","\n","    def triplet(self, score_mat):\n","\n","        true_score = torch.diagonal(score_mat)\n","        # print(\"true_score:\\n\", true_score)\n","        # print(\"true_score dimension:\\n\", true_score.size())\n","        \n","        reduced_score_mat = score_mat - torch.diag(true_score) # set the diagnoal score to be zero\n","        \n","        neg_index_1 = torch.argmax(reduced_score_mat, dim = 1) # indices of hard negatives for GEX\n","        neg_index_2 = torch.argmax(reduced_score_mat, dim = 0) # indices of hard negatives for ATAC\n","        # print(\"neg_index_1:\\n\", neg_index_1); print(\"neg_index_2:\\n\", neg_index_2)\n","        # print(\"neg_index_1 dimension:\\n\", neg_index_1.size()); print(\"neg_index_2 dimension:\\n\", neg_index_2.size())\n","        neg_1 = score_mat[[range(config.BATCH_SIZE), neg_index_1]] # hard negatives for GEX \n","        neg_2 = score_mat[[neg_index_2, range(config.BATCH_SIZE)]] # hard negatives for ATAC\n","        # print(\"neg_1:\\n\", neg_1); print(\"neg_2:\\n\", neg_2)\n","        \n","        loss_1 = torch.max(self.margin - true_score + neg_1, torch.zeros(1, config.BATCH_SIZE).to(config.DEVICE))\n","        loss_2 = torch.max(self.margin - true_score + neg_2, torch.zeros(1, config.BATCH_SIZE).to(config.DEVICE))\n","\n","        return torch.mean(loss_1 + loss_2)\n","\n","    def crossEntropy(self, score_mat):\n","\n","        batch_size = score_mat.size()[0]\n","        target = torch.arange(batch_size)\n","\n","        loss_1 = self.cross_entropy_loss(score_mat, target.to(config.DEVICE))\n","        loss_2 = self.cross_entropy_loss(score_mat.T, target.to(config.DEVICE))\n","\n","        return 0.5 * (loss_1 + loss_2)\n","\n","    def cellTypeMatchingProbRow(self, score_mat, cell_type):\n","\n","        # Collect list of index list for each cell type\n","        idx_in_type = collections.defaultdict(list)\n","        for i, x in enumerate(cell_type):\n","            idx_in_type[x].append(i)\n","\n","        # Compute matching probs for each cell type\n","        score_mat_norm = score_mat.softmax(dim = 0)\n","        probs = []\n","        for idx in idx_in_type.values():\n","            prob_type = 0\n","            for i in idx:\n","                prob_type += score_mat_norm[i, idx].sum()\n","            probs.append(prob_type / len(idx)) \n","\n","        # Take average of matching prob from cell types\n","        ct_match_prob = torch.tensor(probs).mean()\n","\n","        return ct_match_prob\n","\n","    def cellTypeMatchingProb(self, score_mat, cell_type):\n","        row = self.cellTypeMatchingProbRow(score_mat, cell_type) # Softmax on rows (normalize GEX)\n","        col = self.cellTypeMatchingProbRow(score_mat.T, cell_type) # Softmax on cols (normalize ATAC) \n","        \n","        return 0.5 * (row + col)\n","\n","    def forward(self, gex_out_0, gex_out_1, atac_out_0, atac_out_1, cell_type):\n","      \n","        score_mat = self.similarityScore(gex_out_0, gex_out_1, atac_out_0, atac_out_1)#; print(\"score_mat:\\n\", score_mat)\n","\n","        loss_triplet = self.triplet(score_mat)\n","        loss_cross = self.crossEntropy(score_mat)\n","        loss = loss_triplet + loss_cross\n","        # print(score_mat); print(cell_type)\n","        ct_match_prob = self.cellTypeMatchingProb(score_mat, cell_type)\n","\n","        return loss.to(config.DEVICE), loss_triplet, loss_cross, ct_match_prob"]},{"cell_type":"code","source":["# sum_score_mat=torch.zeros(len(idx_in_type.values()),len(idx_in_type.values()))\n","#     for i, dx in enumerate(idx_in_type.values()):\n","#       for j, dx2 in enumerate(idx_in_type.values()):\n","#         tem=score_mat[np.ix_(dx, dx2)].sum()\n","#         print(tem)\n","#         sum_score_mat[i,j]=tem\n","#     print(\"score_mat before softmax\")\n","#     print(sum_score_mat)\n","#     score_mat_norm = 0.5 * (sum_score_mat.softmax(dim = 0) + sum_score_mat.softmax(dim = 1))\n","#     print(sum_score_mat)\n","#     #return torch.tensor(probs).mean()\n","#     return np.sum(np.diagonal(score_mat_norm))"],"metadata":{"id":"V5lkpftQX9kM","executionInfo":{"status":"ok","timestamp":1670991128914,"user_tz":480,"elapsed":10,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670991128914,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"B3QiH33JZGL3"},"outputs":[],"source":["# TEST\n","# import random\n","# random.seed(0)\n","# gex_mat=torch.randn([5,64])\n","# # print(gex_mat[0,])\n","# atac_mat=torch.randn([5,64])\n","# # print(atac_mat[0,])\n","# # print(torch.dot(gex_mat[0,:32],atac_mat[0,:32]))\n","# loss=bidirectTripletLoss(alpha=0.2,margin=1)\n","# res=loss(gex_mat,atac_mat)\n","# res"]},{"cell_type":"code","source":["# def cellTypeMatchingProbRow(score_mat, cell_type):\n","\n","#     # Collect list of index list for each cell type\n","#     idx_in_type = collections.defaultdict(list)\n","#     for i, x in enumerate(cell_type):\n","#         idx_in_type[x].append(i)\n","\n","#     # Compute matching probs for each cell type\n","#     score_mat_norm = score_mat.softmax(dim = 0)\n","#     print('score_mat_norm: \\n', score_mat_norm)\n","#     probs = []\n","#     for idx in idx_in_type.values():\n","#         print('idx: ', idx)\n","#         prob_type = 0\n","#         for i in idx:\n","#             prob_type += score_mat_norm[i, idx].sum()\n","#         print('prob_type: ', prob_type)\n","#         probs.append(prob_type / len(idx)) \n","#     print('probs: ', probs, \"\\n\")\n","\n","#     # Take average of matching prob from cell types\n","#     ct_match_prob = torch.tensor(probs).mean()\n","\n","#     return ct_match_prob\n","\n","# def cellTypeMatchingProb(score_mat, cell_type):\n","#     row = cellTypeMatchingProbRow(score_mat, cell_type) # Softmax on rows (normalize GEX)\n","#     col = cellTypeMatchingProbRow(score_mat.T, cell_type) # Softmax on cols (normalize ATAC) \n","    \n","#     return 0.5 * (row + col)\n","\n","# random.seed(1)\n","# mat=torch.randn([5,5])\n","# cell_type=['a','a','b','a','b']\n","# print(mat)\n","# print(cellTypeMatchingProb(mat, cell_type))"],"metadata":{"id":"zpMYKKtHUYa5","executionInfo":{"status":"ok","timestamp":1670991128914,"user_tz":480,"elapsed":9,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670991128915,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"InmmdcllfCcR"},"outputs":[],"source":["class MultiomeDataset(Dataset):\n","    def __init__(\n","        self, csr_gex, csr_atac, cell_type\n","    ):\n","        super().__init__()\n","        \n","        self.csr_gex = csr_gex\n","        self.csr_atac = csr_atac\n","    \n","    def __len__(self):\n","        return self.csr_gex.shape[0]\n","    \n","    def __getitem__(self, index: int):\n","        x_gex = torch.tensor(self.csr_gex[index,:].todense())\n","        x_atac = torch.tensor(self.csr_atac[index,:].todense())\n","        return {'gex':x_gex, 'atac':x_atac, 'cell_type':cell_type[index]}\n","  \n","def get_dataloaders(gex_train, atac_train, cell_type_train,  gex_val, atac_val, cell_type_val):\n","    \n","    # mod2_train = mod2_train.iloc[sol_train.values.argmax(1)]\n","    # mod2_test = mod2_test.iloc[sol_test.values.argmax(1)]\n","    \n","    dataset_train = MultiomeDataset(gex_train, atac_train, cell_type_train)\n","    data_train = DataLoader(dataset_train, config.BATCH_SIZE, shuffle = True, num_workers = config.NUM_WORKERS)\n","    \n","    dataset_val = MultiomeDataset(gex_val, atac_val, cell_type_val)\n","    data_val = DataLoader(dataset_val, 1024, shuffle = False, num_workers = config.NUM_WORKERS)\n","    \n","    return data_train, data_val"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"BO9Ki-cAkNmn","executionInfo":{"status":"ok","timestamp":1670991169684,"user_tz":480,"elapsed":40779,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}}},"outputs":[],"source":["index = get_chr_index(ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/ATAC_processed.h5ad\"))"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"77cZW1_VbJhr","executionInfo":{"status":"ok","timestamp":1670991207810,"user_tz":480,"elapsed":38150,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}}},"outputs":[],"source":["batch = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\").obs['batch']\n","batch = list(batch)\n","train_id = [a for a, l in enumerate(batch) if l not in ['s2d4','s1d1']]\n","val_id =  [a for a, l in enumerate(batch) if l == 's1d1']\n","test_id = [a for a, l in enumerate(batch) if l == 's2d4']\n","\n","cell_type = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\").obs['cell_type']\n","\n","csr_gex = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\").layers['log_norm']\n","csr_atac = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/ATAC_processed.h5ad\").layers['log_norm']"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"S4JBW6Qakk0k","executionInfo":{"status":"ok","timestamp":1670991207812,"user_tz":480,"elapsed":7,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}}},"outputs":[],"source":["import random\n","random.seed(0)\n","\n","idx_train = [train_id[i] for i in random.sample(range(0, 45000), 10240)]\n","gex_train = csr_gex[idx_train,:]\n","atac_train = csr_atac[idx_train,:]\n","cell_type_train = [cell_type[j] for j in idx_train]\n","\n","idx_val = [train_id[i] for i in random.sample(range(0, 6000), 100)]\n","gex_val = csr_gex[idx_val,:]\n","atac_val = csr_atac[idx_val,:]\n","cell_type_val = [cell_type[j] for j in idx_val]\n","\n","data_train, data_val = get_dataloaders(gex_train, atac_train, cell_type_train, gex_val, atac_val, cell_type_val)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"iGKAQspKvc_V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670991212089,"user_tz":480,"elapsed":4283,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}},"outputId":"c3d01f38-f478-46b2-c2f4-714ca3998df2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n","  warnings.warn('Lazy modules are a new feature under heavy development '\n"]}],"source":["criterion = bidirectTripletLoss(alpha = 0.2, margin = 0.5).to(config.DEVICE)\n","model = Encoder(kernel_size_gex = 100, kernel_size_atac_1 = 50, kernel_size_atac_2 = 10, index = index).to(config.DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr = config.LEARNING_RATE)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.1)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [100, 300], gamma = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BE0KBuDa7mH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"abdd13ba-aba1-4130-aa9b-6d91be75e143"},"outputs":[{"output_type":"stream","name":"stdout","text":["triplet_loss =  tensor(1.6658, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(4.0139, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.5213, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.8324, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.5196, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.7898, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.4283, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.7082, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.3625, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.6737, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.3043, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5969, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.3251, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.6090, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.2390, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5696, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.3302, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5994, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.2254, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5515, device='cuda:0', grad_fn=<MulBackward0>)\n","Epoch-10: lr = 5e-05, loss = 3.5903762876987457, triplet loss = 1.2953081130981445, cell type match prob = 0.08192484080791473\n","triplet_loss =  tensor(1.2366, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5374, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.2971, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5592, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.1942, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5484, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.2337, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5329, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.1992, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5118, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.2335, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5114, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.2772, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5267, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.1517, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5065, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.1877, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.4744, device='cuda:0', grad_fn=<MulBackward0>)\n","triplet_loss =  tensor(1.1854, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.5065, device='cuda:0', grad_fn=<MulBackward0>)\n","Epoch-20: lr = 5e-05, loss = 3.5016008466482162, triplet loss = 1.2107619047164917, cell type match prob = 0.0840039774775505\n"]}],"source":["def train(model, data_train, epochs):\n","\n","  model.train()\n","\n","  for e in range(epochs):\n","    running_loss = 0.0\n","    running_loss_triplet = 0.0\n","    running_ct_prob = 0.0\n","    for iter, data in enumerate(data_train):\n","      gex_input = data['gex'].to(config.DEVICE)\n","      atac_input = data['atac'].to(config.DEVICE)\n","      cell_type_input = data['cell_type']\n","      # print(cell_type_input)\n","\n","      model.zero_grad()\n","      optimizer.zero_grad()\n","\n","      ### Forward\n","      gex_out_0, gex_out_1, atac_out_0, atac_out_1 = model(gex_input, atac_input)\n","\n","      ### Compute loss\n","      loss, loss_triplet, loss_cross, ct_match_prob = criterion(gex_out_0, gex_out_1, atac_out_0, atac_out_1, cell_type_input)\n","      \n","      ### Propagate loss\n","      # running_loss += loss.item()\n","      # loss.backward()\n","      running_loss += loss_cross.item()\n","      loss_cross.backward()\n","\n","      running_ct_prob += ct_match_prob\n","      running_loss_triplet += loss_triplet\n","\n","      ### update parameters\n","      optimizer.step()\n","    \n","    # scheduler.step()\n","    \n","    print('triplet_loss = ', loss_triplet)\n","    print('cross_loss = ', loss_cross)\n","    if (e+1) % 10 == 0: \n","      print('Epoch-{0}: lr = {1}, loss = {2}, triplet loss = {3}, cell type match prob = {4}'.format(\n","          e+1, \n","          optimizer.param_groups[0]['lr'], \n","          running_loss.item() / len(data_train), \n","          running_loss_triplet.item() / len(data_train), \n","          running_ct_prob.item() / len(data_train)\n","          )\n","      )\n","\n","train(model, data_train, epochs = 500)"]},{"cell_type":"code","source":["# train(model, data_train, epochs = 200)"],"metadata":{"id":"i1sVT3lsf9bp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def train(model, data_train, epochs):\n","\n","#   model.train()\n","\n","#   for e in range(epochs):\n","#     running_loss = 0.0\n","#     for iter, data in enumerate(data_train):\n","#       gex_input = data['gex'].to(config.DEVICE)\n","#       atac_input = data['atac'].to(config.DEVICE)\n","\n","#       model.zero_grad()\n","#       optimizer.zero_grad()\n","\n","#       # forward\n","#       gex_out_0, gex_out_1, atac_out_0, atac_out_1 = model(gex_input, atac_input)\n","#       # print(gex_out_0); print(gex_out_1); print(atac_out_0); print(atac_out_1); \n","\n","#       loss, loss_triplet, loss_cross = criterion(gex_out_0, gex_out_1, atac_out_0, atac_out_1)\n","      \n","#       # running_loss += loss.item()\n","#       # loss.backward()\n","#       running_loss += loss_triplet.item()\n","#       loss_triplet.backward()\n","\n","#       # update parameters\n","#       optimizer.step()\n","    \n","#     # scheduler.step()\n","    \n","#     print('triplet_loss = ', loss_triplet); print('cross_loss = ', loss_cross)\n","#     if (e+1) % 10 == 0: \n","#       # print('Epoch-{0} lr: {1}'.format(e+1, optimizer.param_groups[0]['lr']))\n","#       print('epoch[%d] = %.8f' % (e+1, running_loss / len(data_train)))\n","\n","\n","# train(model, data_train, epochs = 100)"],"metadata":{"id":"2JHJplAHx32K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train(model, data_train, epochs = 100)"],"metadata":{"id":"NR1ie2ZBUe47"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5cgSa2rj6R3"},"outputs":[],"source":["torch.save(model.state_dict(), 'drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_10240cells')"]},{"cell_type":"code","source":[],"metadata":{"id":"hfg4rUCXzAQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZT8AmAZPzATx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VDc_UOurzAWl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cp0XMvymzAZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Jwwf5xnnzAbo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def inference(model, data_val):\n","\n","    # Initialize encoder & decoder \n","    model.eval()\n","    model.to('cpu')\n","    criterion.to('cpu')\n","    \n","    for iter, data in enumerate(data_val):\n","      gex_input = data['gex'].to('cpu')\n","      atac_input = data['atac'].to('cpu')\n","      cell_type_input = data['cell_type']\n","\n","      print(collections.Counter(cell_type_input))\n","\n","      ### Forward\n","      gex_out_0, gex_out_1, atac_out_0, atac_out_1 = model(gex_input, atac_input)\n","\n","      ### Compute loss\n","      loss, loss_triplet, loss_cross, ct_match_prob = criterion(gex_out_0, gex_out_1, atac_out_0, atac_out_1, cell_type_input)\n","\n","    return loss, loss_triplet, loss_cross, ct_match_prob\n","\n"],"metadata":{"id":"DIgJkWVD76SF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Encoder(kernel_size_gex = 100, kernel_size_atac_1 = 50, kernel_size_atac_2 = 10, index = index).to(config.DEVICE)\n","criterion = bidirectTripletLoss(alpha = 0.2, margin = 0.5).to(config.DEVICE)\n","model.load_state_dict(torch.load('drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_10240cells'))\n","loss, loss_triplet, loss_cross, ct_match_prob = inference(model, data_val)"],"metadata":{"id":"VKQlEJeowARe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('loss = {0}, triplet loss = {1}, cell type match prob = {2}'.format(loss, loss_triplet, ct_match_prob))"],"metadata":{"id":"DAZAeIYtzOR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fCXfdLlU0a44"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}