{"cells":[{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1088,"status":"ok","timestamp":1670983050797,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"khP6NWrdKWSk","outputId":"d5a8710a-7eeb-4e79-fad0-9bd87d94ce0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":3031,"status":"ok","timestamp":1670983054008,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"SZFSNox2KiKN"},"outputs":[],"source":["!pip install scanpy --quiet"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1670983054008,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"7bo5YGlE7wI6"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.autograd import Variable\n","import anndata as ad\n","import numpy as np\n","import os\n","import collections \n","from argparse import Namespace\n","from torch.utils.data import Dataset, DataLoader\n","config = Namespace(\n","    DEVICE = 'cuda',\n","    BATCH_SIZE = 64,\n","    NUM_WORKERS = 4,\n","    N_GENES = 13431,\n","    N_PEAKS = 116465,\n","    N_CHANNELS = 128,\n","    MAX_SEQ_LEN_GEX = 1500,\n","    MAX_SEQ_LEN_ATAC = 15000,\n","    LEARNING_RATE = 0.0005\n",")"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1670983054009,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"xDWcM83eKMAJ"},"outputs":[],"source":["# adata_gex = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\")"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1670983054009,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"HyD0b7IGp412"},"outputs":[],"source":["# adata_atac = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/ATAC_processed.h5ad\")"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":231,"status":"ok","timestamp":1670983054233,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"rcYnPeRzsPLH"},"outputs":[],"source":["def get_chr_index(adata_atac):\n","  r\"\"\"\n","  Output row indices for each chromosome for each chromosome\n","  Parameters\n","  ----------\n","  adata_atac\n","      annData for ATAC\n","  Returns\n","  -------\n","  chr_index\n","      Dictionary of indices for each chromosome\n","  \"\"\"\n","  row_name = adata_atac.var.index\n","  chr_name = [c.split(\"-\")[0] for c in row_name]\n","  lst = np.unique(chr_name) # names for chromosome\n","\n","  chr_index = dict()\n","  for i in range(len(lst)):\n","    index = [a for a, l in enumerate(chr_name) if l == lst[i]]\n","    if lst[i] not in chr_index:\n","      chr_index[lst[i]]=index\n","\n","  return chr_index"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670983054234,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"FlJMBCpbKIEa"},"outputs":[],"source":["## Write cnn modules for gex modalities\n","class gexCNN(nn.Module):\n","    \"\"\"customized  module\"\"\"\n","    #argument index is the poisition for each choromosome\n","    def __init__(self, kernel_size):\n","        super(gexCNN, self).__init__()\n","\n","        # Conv layer\n","        self.in_channels = 1 \n","        self.out_channels = config.N_CHANNELS\n","        self.kernel_size = kernel_size   \n","        self.stride = 50 # TO CHANGE \n","        self.padding = 25 # TO CHANGE\n","        self.pool_size = 2\n","        self.pool_stride = 1\n","        self.convs = nn.Sequential(\n","            nn.Conv1d(in_channels = self.in_channels, \n","                      out_channels = self.out_channels, \n","                      kernel_size = self.kernel_size,\n","                      stride = self.stride,\n","                      padding = self.padding),\n","            nn.LeakyReLU(),\n","            nn.MaxPool1d(kernel_size = self.pool_size,\n","                         stride = self.pool_stride)\n","        )\n","\n","        # # FC layer\n","        # self.conv_out_features = int((config.N_GENES + 2*self.padding - self.kernel_size) / self.stride + 1)\n","        # self.fc_in_features = int((self.conv_out_features - self.pool_size) / self.pool_stride + 1) * self.out_channels\n","        # self.fc_out_feature = 300\n","        # self.fc = nn.Linear(in_features = self.fc_in_features, out_features = self.fc_out_feature) \n","\n","    def forward(self, x):\n","        r\"\"\"  \n","        Generate GEX embeddings\n","        \n","        Parameters\n","        ----------\n","        x\n","            Pre-processed GEX data (batch_size x 1 x N_GENES)\n","        \n","        Returns\n","        -------\n","        gex_embed\n","            GEX embeddings of a batch (batch_size x seq_len x dim_size)\n","        \"\"\"\n","        gex_embed = self.convs(x.float())\n","        # gex_embed = torch.flatten(gex_embed, 1)\n","        # gex_embed = self.fc(gex_embed)\n","        return gex_embed.transpose(1,2).to(config.DEVICE)"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670983054234,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"_Fd0OvPg3PGo"},"outputs":[],"source":["# # Test for gexCNN()\n","# x = torch.tensor(np.asarray(csr_gex[:5].todense())).unsqueeze(1) # 5 cells\n","# print(x.size())\n","# model = gexCNN(kernel_size = 100)\n","# print(model(x).size())"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1670983054234,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"LUWvJUcpKIHe"},"outputs":[],"source":["# Write cnn modules for atac modalities\n","class atacCNN(nn.Module):\n","    #argument index is the poisition for each choromosome\n","    def __init__(self, index, kernel_size_1, kernel_size_2):\n","        super(atacCNN, self).__init__()\n","        self.index = index\n","        \n","        # Conv layer\n","        self.in_channels_1 = 1 \n","        self.out_channels_1 = int(config.N_CHANNELS / 2)\n","        self.kernel_size_1 = kernel_size_1\n","        self.stride_1 = 10 # TO CHANGE \n","        self.padding_1 = 5 # TO CHANGE\n","\n","        self.in_channels_2 = int(config.N_CHANNELS / 2)\n","        self.out_channels_2 = config.N_CHANNELS \n","        self.kernel_size_2 = kernel_size_2\n","        self.stride_2 = 5 # TO CHANGE \n","        self.padding_2 = 3 # TO CHANGE\n","\n","        self.convs = nn.Sequential(\n","            nn.Conv1d(in_channels = self.in_channels_1, \n","                      out_channels = self.out_channels_1, \n","                      kernel_size = self.kernel_size_1,\n","                      stride = self.stride_1,\n","                      padding = self.padding_1),\n","            nn.LeakyReLU(),\n","            nn.MaxPool1d(kernel_size = 5, stride = 2),\n","\n","            nn.Conv1d(in_channels = self.in_channels_2, \n","                      out_channels = self.out_channels_2, \n","                      kernel_size = self.kernel_size_2,\n","                      stride = self.stride_2,\n","                      padding = self.padding_2),\n","            nn.LeakyReLU(),\n","            nn.MaxPool1d(kernel_size = 2, stride = 1)             \n","        )\n","\n","\n","\n","    def forward(self, x):\n","        r\"\"\"  \n","        Generate ATAC embeddings\n","        \n","        Parameters\n","        ----------\n","        x\n","            Pre-processed ATAC data (batch_size x 1 x N_PEAKS)\n","        \n","        Returns\n","        -------\n","        atac_embed\n","            ATAC embeddings of a batch (batch_size x seq_len x dim_size)\n","        \"\"\"\n","        atac_embed = []\n","        for chr in self.index.keys(): \n","            idx = self.index[chr]\n","            x_chr = x[:,:,idx]\n","            x_chr = self.convs(x_chr.float())\n","            atac_embed.append(x_chr)\n","        atac_embed = torch.cat(atac_embed, dim = 2)\n","        return atac_embed.transpose(1,2).to(config.DEVICE)"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670983054235,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"orbL3zsTqYPy"},"outputs":[],"source":["# # Test for ATAC_CNN()\n","# x = torch.tensor(np.asarray(csr_atac[:5].todense())).unsqueeze(1) # 5 cells\n","# print(x.size())\n","# # index = get_chr_index(adata_atac)\n","# model = atacCNN(kernel_size_1 = 50, kernel_size_2 = 10, index = index)\n","# print(model(x).size())"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670983054235,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"2cTz8oUD4aGx"},"outputs":[],"source":["class MultimodalAttention(nn.Module):\n","    def __init__(self):\n","        super(MultimodalAttention, self).__init__()\n","        self.nhead_gex = 1\n","        self.nhead_atac = 4\n","        self.nhead_multi = 4\n","        self.nlayer_gex = 1\n","        self.nlayer_atac = 1\n","        self.nlayer_multi = 1\n","\n","        self.encoder_layer_gex = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_gex)\n","        self.transformer_encoder_gex = nn.TransformerEncoder(self.encoder_layer_gex, num_layers = self.nlayer_gex)\n","        self.linear_gex_0 = nn.LazyLinear(out_features = 1)\n","\n","        self.encoder_layer_atac = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_atac)\n","        self.transformer_encoder_atac = nn.TransformerEncoder(self.encoder_layer_atac, num_layers = self.nlayer_atac)\n","        self.linear_atac_0 = nn.LazyLinear(out_features = 1)\n","\n","        self.encoder_layer_multi = nn.TransformerEncoderLayer(d_model = config.N_CHANNELS, nhead = self.nhead_multi)\n","        self.transformer_encoder_multi = nn.TransformerEncoder(self.encoder_layer_multi, num_layers = self.nlayer_multi)\n","        self.linear_gex_1 = nn.LazyLinear(out_features = 1)\n","        self.linear_atac_1 = nn.LazyLinear(out_features = 1)\n","    \n","\n","    def forward(self, gex_embed, atac_embed):\n","      r\"\"\"  \n","      Incorporate two self-attention and one cross-attention module\n","\n","      Parameters\n","      ----------\n","      gex_embed\n","          GEX embeddings of a batch (batch_size x seq_len_gex x dim_size)\n","      atac_embed\n","          ATAC embeddings of a batch (batch_size x seq_len_atac x dim_size)\n","\n","      Returns\n","      -------\n","      ## TO FILL\n","      \"\"\"\n","      seq_len_gex = gex_embed.size()[1]\n","      seq_len_atac = atac_embed.size()[1]\n","      # print(gex_embed.size())\n","      # print(atac_embed.size())\n","\n","      gex_context = self.transformer_encoder_gex(gex_embed)\n","      atac_context = self.transformer_encoder_atac(atac_embed)\n","\n","      # # Average self-attention fragment representation\n","      # gex_out_0 = gex_context.mean(dim = 1)\n","      # atac_out_0 = atac_context.mean(dim = 1)\n","      gex_out_0 = self.linear_gex_0(gex_context.permute(0,2,1)).squeeze(2)\n","      atac_out_0 = self.linear_atac_0(atac_context.permute(0,2,1)).squeeze(2)\n","\n","      multi_embed = torch.cat((gex_context, atac_context), dim = 1)\n","      multi_context = self.transformer_encoder_multi(multi_embed)\n","      # print(multi_context.size())\n","      \n","      multi_context_gex = multi_context[:, :seq_len_gex, :]\n","      multi_context_atac = multi_context[:, seq_len_gex:, :]\n","\n","      # # Average cross-attention fragment representation\n","      # gex_out_1 = multi_context_gex.mean(dim = 1)\n","      # atac_out_1 = multi_context_atac.mean(dim = 1)\n","      gex_out_1 = self.linear_gex_1(multi_context_gex.permute(0,2,1)).squeeze(2)\n","      atac_out_1 = self.linear_atac_1(multi_context_atac.permute(0,2,1)).squeeze(2)\n","\n","      return gex_out_0.to(config.DEVICE), gex_out_1.to(config.DEVICE), atac_out_0.to(config.DEVICE), atac_out_1.to(config.DEVICE)"]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670983054235,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"bKfe5KqplRBE"},"outputs":[],"source":["# # index = get_chr_index(adata_atac)\n","\n","# x_gex = torch.tensor(np.asarray(csr_gex[:5].todense())).unsqueeze(1).to(config.DEVICE) # 5 cells\n","# x_atac = torch.tensor(np.asarray(csr_atac[:5].todense())).unsqueeze(1).to(config.DEVICE) # 5 cells\n","\n","# gex_cnn = gexCNN(kernel_size = 100).to(config.DEVICE)\n","# atac_cnn = atacCNN(kernel_size_1 = 50, kernel_size_2 = 10, index = index).to(config.DEVICE)\n","# multi_attention = MultimodalAttention().to(config.DEVICE)\n","\n","# gex_embed = gex_cnn(x_gex).to(config.DEVICE)\n","# atac_embed = atac_cnn(x_atac).to(config.DEVICE)\n","\n","# gex_out_0, gex_out_1, atac_out_0, atac_out_1 = multi_attention(gex_embed, atac_embed)\n","# print(atac_out_0.size())"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1670983054235,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"jpHCCxVo-1Bj"},"outputs":[],"source":["# embedding = nn.Embedding(1000, 128)\n","# anchor_ids = torch.randint(0, 1000, (1,))\n","# positive_ids = torch.randint(0, 1000, (1,))\n","# negative_ids = torch.randint(0, 1000, (1,))\n","# anchor = embedding(anchor_ids)\n","# positive = embedding(positive_ids)\n","# negative = embedding(negative_ids)"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670983054236,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"5oscyv8g_gPF"},"outputs":[],"source":["# print(anchor.size())\n","# print(positive.size())\n","# print(negative.size())"]},{"cell_type":"code","execution_count":62,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670983054236,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"ZXU1s_kVtKUQ"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, kernel_size_gex, kernel_size_atac_1, kernel_size_atac_2, index):\n","        super(Encoder, self).__init__()\n","\n","        self.kernel_size_gex = kernel_size_gex\n","        self.kernel_size_atac_1 = kernel_size_atac_1\n","        self.kernel_size_atac_2 = kernel_size_atac_2\n","        self.index = index\n","\n","        self.gex_cnn = gexCNN(kernel_size = self.kernel_size_gex)\n","        self.atac_cnn = atacCNN(kernel_size_1 = self.kernel_size_atac_1, kernel_size_2 = self.kernel_size_atac_2, index = self.index)\n","        self.multi_attention = MultimodalAttention()\n","\n","        \n","    def forward(self, x_gex, x_atac):\n","\n","        gex_embed = self.gex_cnn(x_gex)\n","        atac_embed = self.atac_cnn(x_atac)\n","        gex_out_0, gex_out_1, atac_out_0, atac_out_1 = self.multi_attention(gex_embed, atac_embed)\n","\n","        return gex_out_0, gex_out_1, atac_out_0, atac_out_1\n"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670983054236,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"Ql7ebMHlvjho"},"outputs":[],"source":["# gex_test = torch.tensor(np.asarray(adata_gex.layers['log_norm'][:5].todense())).unsqueeze(1) # 5 cells\n","# atac_test = torch.tensor(np.asarray(adata_atac.layers['log_norm'][:5].todense())).unsqueeze(1) # 5 cells\n","\n","# index = get_chr_index(adata_atac)\n"]},{"cell_type":"code","execution_count":64,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670983054236,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"O8JVH48uxfQU"},"outputs":[],"source":["# encoder = Encoder(kernel_size = 32, index = index )\n","# gex_out_0, gex_out_1, atac_out_0, atac_out_1 = encoder(gex_test, atac_test)"]},{"cell_type":"code","execution_count":65,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1670983054236,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"Aq9aH7Nc_d51"},"outputs":[],"source":["from numpy.lib.shape_base import row_stack\n","class bidirectTripletLoss(nn.Module):\n","    r\"\"\"\n","    \n","    Output bidirectional triplet loss for two pairs of gex and atac\n","    ----------\n","    gex_0_mat: Matrix of GEX embeddings from self-attention (batch_size x embedding_size_0)\n","\n","    Returns\n","    -------\n","    loss\n","    \"\"\"\n","    def __init__(self, alpha, margin):\n","        super(bidirectTripletLoss, self).__init__()\n","\n","        self.alpha = alpha\n","        self.margin = margin\n","        self.cross_entropy_loss = nn.CrossEntropyLoss()\n","\n","    def similarityScore(self, gex_out_0, gex_out_1, atac_out_0, atac_out_1):\n","        r\"\"\"\n","        Output similarity scores for two pairs of gex and atac\n","        ----------\n","\n","\n","        Returns\n","        -------\n","        score: batch_size * batch_size\n","        similarity score between two modalities\n","        \"\"\" \n","\n","        # print(gex_mat.size())\n","        # print(atac_mat.size())\n","\n","        # gex_mat0, gex_mat1 = torch.split(gex_mat, config.N_CHANNELS, dim = 1)\n","        # atac_mat0, atac_mat1 = torch.split(atac_mat,config.N_CHANNELS, dim = 1)\n","\n","        gex_out_0 = nn.functional.normalize(gex_out_0, dim = 1)\n","        gex_out_1 = nn.functional.normalize(gex_out_1, dim = 1)\n","        atac_out_0 = nn.functional.normalize(atac_out_0, dim = 1)\n","        atac_out_1 = nn.functional.normalize(atac_out_1, dim = 1)\n","        \n","        score = torch.mm(gex_out_0, atac_out_0.transpose(0,1)) + self.alpha * torch.mm(gex_out_1, atac_out_1.transpose(0,1))\n","        return  score.to(config.DEVICE)\n","\n","    def triplet(self, score_mat):\n","\n","        true_score = torch.diagonal(score_mat)\n","        # print(\"true_score:\\n\", true_score)\n","        # print(\"true_score dimension:\\n\", true_score.size())\n","        \n","        reduced_score_mat = score_mat - torch.diag(true_score) # set the diagnoal score to be zero\n","        \n","        neg_index_1 = torch.argmax(reduced_score_mat, dim = 1) # indices of hard negatives for GEX\n","        neg_index_2 = torch.argmax(reduced_score_mat, dim = 0) # indices of hard negatives for ATAC\n","        # print(\"neg_index_1:\\n\", neg_index_1); print(\"neg_index_2:\\n\", neg_index_2)\n","        # print(\"neg_index_1 dimension:\\n\", neg_index_1.size()); print(\"neg_index_2 dimension:\\n\", neg_index_2.size())\n","        neg_1 = score_mat[[range(config.BATCH_SIZE), neg_index_1]] # hard negatives for GEX \n","        neg_2 = score_mat[[neg_index_2, range(config.BATCH_SIZE)]] # hard negatives for ATAC\n","        # print(\"neg_1:\\n\", neg_1); print(\"neg_2:\\n\", neg_2)\n","        \n","        loss_1 = torch.max(self.margin - true_score + neg_1, torch.zeros(1, config.BATCH_SIZE).to(config.DEVICE))\n","        loss_2 = torch.max(self.margin - true_score + neg_2, torch.zeros(1, config.BATCH_SIZE).to(config.DEVICE))\n","\n","        return torch.mean(loss_1 + loss_2)\n","\n","    def crossEntropy(self, score_mat):\n","\n","        batch_size = score_mat.size()[0]\n","        target = torch.arange(batch_size)\n","\n","        loss_1 = self.cross_entropy_loss(score_mat, target.to(config.DEVICE))\n","        loss_2 = self.cross_entropy_loss(score_mat.T, target.to(config.DEVICE))\n","\n","        return 0.5 * (loss_1 + loss_2)\n","\n","    def cellTypeMatchingProbRow(self, score_mat, cell_type):\n","\n","        # Collect list of index list for each cell type\n","        idx_in_type = collections.defaultdict(list)\n","        for i, x in enumerate(cell_type):\n","            idx_in_type[x].append(i)\n","\n","        # Compute matching probs for each cell type\n","        score_mat_norm = score_mat.softmax(dim = 0)\n","        probs = []\n","        for idx in idx_in_type.values():\n","            prob_type = 0\n","            for i in idx:\n","                prob_type += score_mat_norm[i, idx].sum()\n","            probs.append(prob_type / len(idx)) \n","\n","        # Take average of matching prob from cell types\n","        ct_match_prob = torch.tensor(probs).mean()\n","\n","        return ct_match_prob\n","\n","    def cellTypeMatchingProb(self, score_mat, cell_type):\n","        row = self.cellTypeMatchingProbRow(score_mat, cell_type) # Softmax on rows (normalize GEX)\n","        col = self.cellTypeMatchingProbRow(score_mat.T, cell_type) # Softmax on cols (normalize ATAC) \n","        \n","        return 0.5 * (row + col)\n","\n","    def forward(self, gex_out_0, gex_out_1, atac_out_0, atac_out_1, cell_type):\n","      \n","        score_mat = self.similarityScore(gex_out_0, gex_out_1, atac_out_0, atac_out_1)#; print(\"score_mat:\\n\", score_mat)\n","\n","        loss_triplet = self.triplet(score_mat)\n","        loss_cross = self.crossEntropy(score_mat)\n","        loss = loss_triplet + loss_cross\n","        # print(score_mat); print(cell_type)\n","        ct_match_prob = self.cellTypeMatchingProb(score_mat, cell_type)\n","\n","        return loss.to(config.DEVICE), loss_triplet, loss_cross, ct_match_prob"]},{"cell_type":"code","execution_count":66,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670983054237,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"B3QiH33JZGL3"},"outputs":[],"source":["# TEST\n","# import random\n","# random.seed(0)\n","# gex_mat=torch.randn([5,64])\n","# # print(gex_mat[0,])\n","# atac_mat=torch.randn([5,64])\n","# # print(atac_mat[0,])\n","# # print(torch.dot(gex_mat[0,:32],atac_mat[0,:32]))\n","# loss=bidirectTripletLoss(alpha=0.2,margin=1)\n","# res=loss(gex_mat,atac_mat)\n","# res"]},{"cell_type":"code","source":["# def cellTypeMatchingProbRow(score_mat, cell_type):\n","\n","#     # Collect list of index list for each cell type\n","#     idx_in_type = collections.defaultdict(list)\n","#     for i, x in enumerate(cell_type):\n","#         idx_in_type[x].append(i)\n","\n","#     # Compute matching probs for each cell type\n","#     score_mat_norm = score_mat.softmax(dim = 0)\n","#     print('score_mat_norm: \\n', score_mat_norm)\n","#     probs = []\n","#     for idx in idx_in_type.values():\n","#         print('idx: ', idx)\n","#         prob_type = 0\n","#         for i in idx:\n","#             prob_type += score_mat_norm[i, idx].sum()\n","#         print('prob_type: ', prob_type)\n","#         probs.append(prob_type / len(idx)) \n","#     print('probs: ', probs, \"\\n\")\n","\n","#     # Take average of matching prob from cell types\n","#     ct_match_prob = torch.tensor(probs).mean()\n","\n","#     return ct_match_prob\n","\n","# def cellTypeMatchingProb(score_mat, cell_type):\n","#     row = cellTypeMatchingProbRow(score_mat, cell_type) # Softmax on rows (normalize GEX)\n","#     col = cellTypeMatchingProbRow(score_mat.T, cell_type) # Softmax on cols (normalize ATAC) \n","    \n","#     return 0.5 * (row + col)\n","\n","# random.seed(1)\n","# mat=torch.randn([5,5])\n","# cell_type=['a','a','b','a','b']\n","# print(mat)\n","# print(cellTypeMatchingProb(mat, cell_type))"],"metadata":{"id":"zpMYKKtHUYa5","executionInfo":{"status":"ok","timestamp":1670983054238,"user_tz":480,"elapsed":9,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","execution_count":68,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670983054238,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"},"user_tz":480},"id":"InmmdcllfCcR"},"outputs":[],"source":["class MultiomeDataset(Dataset):\n","    def __init__(\n","        self, csr_gex, csr_atac, cell_type\n","    ):\n","        super().__init__()\n","        \n","        self.csr_gex = csr_gex\n","        self.csr_atac = csr_atac\n","    \n","    def __len__(self):\n","        return self.csr_gex.shape[0]\n","    \n","    def __getitem__(self, index: int):\n","        x_gex = torch.tensor(self.csr_gex[index,:].todense())\n","        x_atac = torch.tensor(self.csr_atac[index,:].todense())\n","        return {'gex':x_gex, 'atac':x_atac, 'cell_type':cell_type[index]}\n","  \n","def get_dataloaders(gex_train, atac_train, cell_type_train,  gex_val, atac_val, cell_type_val):\n","    \n","    # mod2_train = mod2_train.iloc[sol_train.values.argmax(1)]\n","    # mod2_test = mod2_test.iloc[sol_test.values.argmax(1)]\n","    \n","    dataset_train = MultiomeDataset(gex_train, atac_train, cell_type_train)\n","    data_train = DataLoader(dataset_train, config.BATCH_SIZE, shuffle = True, num_workers = config.NUM_WORKERS)\n","    \n","    dataset_val = MultiomeDataset(gex_val, atac_val, cell_type_val)\n","    data_val = DataLoader(dataset_val, config.BATCH_SIZE, shuffle = False, num_workers = config.NUM_WORKERS)\n","    \n","    return data_train, data_val"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"BO9Ki-cAkNmn","executionInfo":{"status":"ok","timestamp":1670983064229,"user_tz":480,"elapsed":9316,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}}},"outputs":[],"source":["index = get_chr_index(ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/ATAC_processed.h5ad\"))"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"77cZW1_VbJhr","executionInfo":{"status":"ok","timestamp":1670983086012,"user_tz":480,"elapsed":21561,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}}},"outputs":[],"source":["batch = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\").obs['batch']\n","batch = list(batch)\n","train_id = [a for a, l in enumerate(batch) if l not in ['s2d4','s1d1']]\n","val_id =  [a for a, l in enumerate(batch) if l == 's1d1']\n","test_id = [a for a, l in enumerate(batch) if l == 's2d4']\n","\n","cell_type = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\").obs['cell_type']\n","\n","csr_gex = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/GEX_processed.h5ad\").layers['log_norm']\n","csr_atac = ad.read_h5ad(\"drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/data/ATAC_processed.h5ad\").layers['log_norm']"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"S4JBW6Qakk0k","executionInfo":{"status":"ok","timestamp":1670983086197,"user_tz":480,"elapsed":206,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}}},"outputs":[],"source":["import random\n","random.seed(0)\n","\n","idx_train = [train_id[i] for i in random.sample(range(0, 45000), 10240)]\n","gex_train = csr_gex[idx_train,:]\n","atac_train = csr_atac[idx_train,:]\n","cell_type_train = [cell_type[j] for j in idx_train]\n","\n","idx_val = [train_id[i] for i in random.sample(range(0, 6000), 1024)]\n","gex_val = csr_gex[idx_val,:]\n","atac_val = csr_atac[idx_val,:]\n","cell_type_val = [cell_type[j] for j in idx_val]\n","\n","data_train, data_val = get_dataloaders(gex_train, atac_train, cell_type_train, gex_val, atac_val, cell_type_val)"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"iGKAQspKvc_V","executionInfo":{"status":"ok","timestamp":1670983086436,"user_tz":480,"elapsed":241,"user":{"displayName":"Ning Shen","userId":"14865340626844030320"}}},"outputs":[],"source":["criterion = bidirectTripletLoss(alpha = 0.2, margin = 0.5).to(config.DEVICE)\n","model = Encoder(kernel_size_gex = 100, kernel_size_atac_1 = 50, kernel_size_atac_2 = 10, index = index).to(config.DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr = config.LEARNING_RATE)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.1)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [100, 300], gamma = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6BE0KBuDa7mH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5607f77-170e-406c-ea12-6f66c88e5e9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["triplet_loss =  tensor(1.4120, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.6292, device='cuda:0', grad_fn=<MulBackward0>)\n","cell_type_match_prob =  tensor(0.0872)\n","triplet_loss =  tensor(1.2230, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.4763, device='cuda:0', grad_fn=<MulBackward0>)\n","cell_type_match_prob =  tensor(0.0817)\n","triplet_loss =  tensor(1.1990, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.4155, device='cuda:0', grad_fn=<MulBackward0>)\n","cell_type_match_prob =  tensor(0.0922)\n","triplet_loss =  tensor(1.2382, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.4329, device='cuda:0', grad_fn=<MulBackward0>)\n","cell_type_match_prob =  tensor(0.0744)\n","triplet_loss =  tensor(1.2268, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.4182, device='cuda:0', grad_fn=<MulBackward0>)\n","cell_type_match_prob =  tensor(0.0779)\n","triplet_loss =  tensor(1.2065, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.4183, device='cuda:0', grad_fn=<MulBackward0>)\n","cell_type_match_prob =  tensor(0.0873)\n","triplet_loss =  tensor(1.1477, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.3716, device='cuda:0', grad_fn=<MulBackward0>)\n","cell_type_match_prob =  tensor(0.1062)\n","triplet_loss =  tensor(1.1550, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.3726, device='cuda:0', grad_fn=<MulBackward0>)\n","cell_type_match_prob =  tensor(0.0853)\n","triplet_loss =  tensor(1.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.3489, device='cuda:0', grad_fn=<MulBackward0>)\n","cell_type_match_prob =  tensor(0.0812)\n","triplet_loss =  tensor(1.1077, device='cuda:0', grad_fn=<MeanBackward0>)\n","cross_loss =  tensor(3.3529, device='cuda:0', grad_fn=<MulBackward0>)\n","cell_type_match_prob =  tensor(0.0901)\n","Epoch-10 lr: 0.0005\n","epoch[10] = 3.33891112\n"]}],"source":["def train(model, data_train, epochs):\n","\n","  model.train()\n","\n","  for e in range(epochs):\n","    running_loss = 0.0\n","    for iter, data in enumerate(data_train):\n","      gex_input = data['gex'].to(config.DEVICE)\n","      atac_input = data['atac'].to(config.DEVICE)\n","      cell_type_input = data['cell_type']\n","      # print(cell_type_input)\n","\n","      model.zero_grad()\n","      optimizer.zero_grad()\n","\n","      ### Forward\n","      gex_out_0, gex_out_1, atac_out_0, atac_out_1 = model(gex_input, atac_input)\n","\n","      ### Compute loss\n","      loss, loss_triplet, loss_cross, ct_match_prob = criterion(gex_out_0, gex_out_1, atac_out_0, atac_out_1, cell_type_input)\n","      \n","      ### Propagate loss\n","      # running_loss += loss.item()\n","      # loss.backward()\n","      running_loss += loss_cross.item()\n","      loss_cross.backward()\n","\n","      ### update parameters\n","      optimizer.step()\n","    \n","    # scheduler.step()\n","    \n","    print('triplet_loss = ', loss_triplet); print('cross_loss = ', loss_cross); print('cell_type_match_prob = ', ct_match_prob)\n","    if (e+1) % 10 == 0: \n","      print('Epoch-{0} lr: {1}'.format(e+1, optimizer.param_groups[0]['lr']))\n","      print('epoch[%d] = %.8f' % (e+1, running_loss / len(data_train)))\n","\n","\n","train(model, data_train, epochs = 100)"]},{"cell_type":"code","source":["# def train(model, data_train, epochs):\n","\n","#   model.train()\n","\n","#   for e in range(epochs):\n","#     running_loss = 0.0\n","#     for iter, data in enumerate(data_train):\n","#       gex_input = data['gex'].to(config.DEVICE)\n","#       atac_input = data['atac'].to(config.DEVICE)\n","\n","#       model.zero_grad()\n","#       optimizer.zero_grad()\n","\n","#       # forward\n","#       gex_out_0, gex_out_1, atac_out_0, atac_out_1 = model(gex_input, atac_input)\n","#       # print(gex_out_0); print(gex_out_1); print(atac_out_0); print(atac_out_1); \n","\n","#       loss, loss_triplet, loss_cross = criterion(gex_out_0, gex_out_1, atac_out_0, atac_out_1)\n","      \n","#       # running_loss += loss.item()\n","#       # loss.backward()\n","#       running_loss += loss_triplet.item()\n","#       loss_triplet.backward()\n","\n","#       # update parameters\n","#       optimizer.step()\n","    \n","#     # scheduler.step()\n","    \n","#     print('triplet_loss = ', loss_triplet); print('cross_loss = ', loss_cross)\n","#     if (e+1) % 10 == 0: \n","#       # print('Epoch-{0} lr: {1}'.format(e+1, optimizer.param_groups[0]['lr']))\n","#       print('epoch[%d] = %.8f' % (e+1, running_loss / len(data_train)))\n","\n","\n","# train(model, data_train, epochs = 100)"],"metadata":{"id":"2JHJplAHx32K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train(model, data_train, epochs = 100)"],"metadata":{"id":"NR1ie2ZBUe47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oxSBdcDY0aW6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iXB48_bV0aab"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5cgSa2rj6R3"},"outputs":[],"source":["# torch.save(model.state_dict(), 'drive/MyDrive/Colab_Notebooks/CPSC532S/final_project/model/trained_model_10240cells')"]},{"cell_type":"code","source":[],"metadata":{"id":"DIgJkWVD76SF"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}